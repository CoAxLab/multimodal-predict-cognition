{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we run the stacked predictive scenario for each cognitive score and generate the prediction accuracies, as well as the weight contributions of each single-channel to the LASSO stacked model. As explained in the manuscript, out-of-sample predictions have been achieved using a Montecarlo cross-validation with 100 random splits into training (70% of the total data) and test (30% of the total data) set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from os.path import join as opj\n",
    "import sys\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "#Sklearn stuff\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import  (StandardScaler, PolynomialFeatures)\n",
    "from sklearn.linear_model import (Lasso, LassoCV, LinearRegression)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add src folder from which we can import some customed functions\n",
    "sys.path.append(\"/home/javier/Documentos/multimodal-cognition/\")\n",
    "\n",
    "# This is the stacking class\n",
    "from stacking import StackingLassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f =  h5py.File(opj(\"../data\", \"final_data.hdf5\"), \"a\")\n",
    "YY_domain_cognition =  f['YY_domain_cognition'][:]\n",
    "X_conn = f['connectome_features'][:]\n",
    "X_surf = f['surface_features'][:]\n",
    "X_thic = f['thickness_features'][:]\n",
    "X_subv = f['sub_vols_features'][:]\n",
    "X_locc = f['loc_conn_features'][:]\n",
    "subjects = f['subjects'][:]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load age data (restricted, so it's not in the repo) and binarise it in order to generate train/test partitions stratified by age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = pd.read_excel(opj(\"../data\", \"age.xlsx\"))\n",
    "age = pd.merge(pd.DataFrame({'Subject': subjects}), age, on='Subject')['Age_in_Yrs'].values\n",
    "\n",
    "age_bin = np.digitize(age, np.quantile(age, q=np.arange(0.1, 1, 0.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepend_transformation = [VarianceThreshold(), StandardScaler(), PCA()]\n",
    "stack_lasso = StackingLassoCV(prepend_transformation=prepend_transformation, cv=5,\n",
    "                              n_jobs=1, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_names = ['CogTotalComp_Unadj', \n",
    "               'CogFluidComp_Unadj', \n",
    "               'CogCrystalComp_Unadj', \n",
    "               'SCPT_SEN', \n",
    "               'DDisc_AUC_200', \n",
    "               'IWRD_TOT', \n",
    "               'VSPLOT_TC']\n",
    "\n",
    "modality_names = ['CONNECTOME', \n",
    "                  'SURFACE', \n",
    "                  'THICKNESS', \n",
    "                  'SUB_VOLUMES', \n",
    "                  'LOCAL_CONN']\n",
    "n_scores = len(score_names) \n",
    "n_mods = len(modality_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_age_new(YY_train, YY_test, M_train, M_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Function used to adjust age using a simple linear regression\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # We already pass  M with an intercept, so we need to set this here to zero \n",
    "    linReg = LinearRegression(fit_intercept=False) \n",
    "    # Fit using ONLY training data\n",
    "    linReg.fit(M_train, YY_train)\n",
    "    \n",
    "    # Substract predicted values and add intercept to keep the same scale. \n",
    "    # Here the intercept is the first column of the M matrix that we passed\n",
    "    YY_train_adj = YY_train - linReg.predict(M_train) + linReg.coef_[:, 0]\n",
    "    YY_test_adj = YY_test - linReg.predict(M_test) + linReg.coef_[:, 0]\n",
    "    \n",
    "    return YY_train_adj, YY_test_adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_stage_predictions(list_X_train, list_X_test, y_train, y_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Function to generate the single-channel predictions to be\n",
    "    later stacked and feed to a LASSO model\n",
    "    \n",
    "    \"\"\"\n",
    "    prepend_transformation = [VarianceThreshold(), StandardScaler(), PCA()]\n",
    "    stack_lasso = StackingLassoCV(prepend_transformation=prepend_transformation, cv=5,\n",
    "                                  n_jobs=1, random_state = RANDOM_STATE)\n",
    "    \n",
    "    stack_lasso.fit(list_X_train, y_train)\n",
    "\n",
    "    part_train_cv = stack_lasso.stacked_features_\n",
    "\n",
    "    y_stack_pred =  stack_lasso.predict(list_X_test)\n",
    "\n",
    "    part_test_pred = y_stack_pred\n",
    "        \n",
    "    return part_train_cv, part_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced(YY_train, YY_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Given a partition, check whether they are balanced \n",
    "    if the ditribution of the response variables in training and test set \n",
    "    don't differ as measured by a kolmogorosv-smirnov test.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ps = np.array([ks_2samp(YY_train[:,i], YY_test[:, i])[1] for i in range(YY_train.shape[1])])\n",
    "    \n",
    "    if np.all(ps > 0.05):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 100 # Number of random splits in the Montecarlo cross-validation\n",
    "n_train = int(0.7*X_conn.shape[0]) # Size of the training set\n",
    "n_test = X_conn.shape[0] - n_train # Size of the test set\n",
    "n_mods = len(modality_names) # Number of channels\n",
    "n_scores = len(score_names) # Number of response variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate list of training/test set index partitions stratified by age\n",
    "\n",
    "partition_idxs = []\n",
    "# Construct single predictors classifiers\n",
    "i_split = 0\n",
    "seed = 0\n",
    "while i_split < n_splits:\n",
    "    \n",
    "    # Generate partition\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=n_train, random_state=seed)\n",
    "    train_index, test_index = list(sss.split(np.zeros(len(age_bin)), age_bin))[0]\n",
    "    \n",
    "    # Create training and test partitions\n",
    "    YY_train = YY_domain_cognition[train_index]\n",
    "    YY_test = YY_domain_cognition[test_index]\n",
    "    \n",
    "    # If training and test are not balanced, generate a new partition\n",
    "    if balanced(YY_train, YY_test) is False:\n",
    "        seed = seed + 1\n",
    "        continue\n",
    "    \n",
    "    partition_idxs.append((train_index, test_index))\n",
    "    #print(\"split %d finished\\n\" % i_split)\n",
    "    i_split = i_split + 1\n",
    "    seed = seed + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store these partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(opj('../results', 'partitions.hdf5'), 'a') as f_partitions:\n",
    "    f_partitions.create_dataset('training_idxs', \n",
    "                                data = np.array([train for (train, _) in  partition_idxs]).T)\n",
    "    f_partitions.create_dataset('testing_idxs', \n",
    "                                data = np.array([test for (_, test) in  partition_idxs]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 0 finished\n",
      "split 1 finished\n",
      "split 2 finished\n",
      "split 3 finished\n",
      "split 4 finished\n",
      "split 5 finished\n",
      "split 6 finished\n",
      "split 7 finished\n",
      "split 8 finished\n",
      "split 9 finished\n",
      "split 10 finished\n",
      "split 11 finished\n",
      "split 12 finished\n",
      "split 13 finished\n",
      "split 14 finished\n",
      "split 15 finished\n",
      "split 16 finished\n",
      "split 17 finished\n",
      "split 18 finished\n",
      "split 19 finished\n",
      "split 20 finished\n",
      "split 21 finished\n",
      "split 22 finished\n",
      "split 23 finished\n",
      "split 24 finished\n",
      "split 25 finished\n",
      "split 26 finished\n",
      "split 27 finished\n",
      "split 28 finished\n",
      "split 29 finished\n",
      "split 30 finished\n",
      "split 32 finished\n",
      "split 33 finished\n",
      "split 34 finished\n",
      "split 35 finished\n",
      "split 36 finished\n",
      "split 37 finished\n",
      "split 38 finished\n",
      "split 39 finished\n",
      "split 40 finished\n",
      "split 41 finished\n",
      "split 42 finished\n",
      "split 43 finished\n",
      "split 44 finished\n",
      "split 45 finished\n",
      "split 46 finished\n",
      "split 47 finished\n",
      "split 48 finished\n",
      "split 49 finished\n",
      "split 50 finished\n",
      "split 51 finished\n",
      "split 52 finished\n",
      "split 53 finished\n",
      "split 54 finished\n",
      "split 55 finished\n",
      "split 56 finished\n",
      "split 57 finished\n",
      "split 58 finished\n",
      "split 59 finished\n",
      "split 60 finished\n",
      "split 61 finished\n",
      "split 62 finished\n",
      "split 63 finished\n",
      "split 64 finished\n",
      "split 65 finished\n",
      "split 66 finished\n",
      "split 67 finished\n",
      "split 68 finished\n",
      "split 69 finished\n",
      "split 70 finished\n",
      "split 71 finished\n",
      "split 72 finished\n",
      "split 73 finished\n",
      "split 74 finished\n",
      "split 75 finished\n",
      "split 76 finished\n",
      "split 77 finished\n",
      "split 78 finished\n",
      "split 79 finished\n",
      "split 80 finished\n",
      "split 81 finished\n",
      "split 82 finished\n",
      "split 83 finished\n",
      "split 84 finished\n",
      "split 85 finished\n",
      "split 86 finished\n",
      "split 87 finished\n",
      "split 88 finished\n",
      "split 89 finished\n",
      "split 90 finished\n",
      "split 91 finished\n",
      "split 92 finished\n",
      "split 93 finished\n",
      "split 94 finished\n",
      "split 95 finished\n",
      "split 96 finished\n",
      "split 97 finished\n",
      "split 98 finished\n",
      "split 99 finished\n"
     ]
    }
   ],
   "source": [
    "# Construct single predictors classifiers\n",
    "i_split = 0\n",
    "seed = 0\n",
    "\n",
    "single_r2_scores = np.zeros((n_splits, n_scores, n_mods))\n",
    "comb_r2_scores = np.zeros((n_splits, n_scores))\n",
    "comb_weights = np.zeros((n_splits, n_scores, n_mods))\n",
    "\n",
    "#Create empty dictionary for the predictions\n",
    "preds_dict = {}\n",
    "for score in score_names:\n",
    "    preds_dict[score] = []\n",
    "    \n",
    "poly =  PolynomialFeatures(degree = 1) \n",
    "\n",
    "#compute starting execution time\n",
    "start = time.time()    \n",
    "for train_index, test_index in partition_idxs:\n",
    "\n",
    "    YY_train = YY_domain_cognition[train_index]\n",
    "    YY_test = YY_domain_cognition[test_index]\n",
    "    \n",
    "    age_train, age_test = age[train_index], age[test_index] \n",
    "    M_train = poly.fit_transform(age_train[:, np.newaxis]) \n",
    "    M_test = poly.transform(age_test[:, np.newaxis]) \n",
    "    \n",
    "    # Adjust by  age \n",
    "    YY_train_adj, YY_test_adj = adjust_age_new(YY_train, YY_test, M_train, M_test)\n",
    "            \n",
    "    for jj, score in enumerate(score_names):\n",
    "            \n",
    "        y_train = YY_train_adj[:, jj]\n",
    "        y_test = YY_test_adj[:, jj]\n",
    "        \n",
    "        # Compute single predictions\n",
    "        X_train_2, X_test_2 = first_stage_predictions([X_conn[train_index], \n",
    "                                                       X_surf[train_index], \n",
    "                                                       X_thic[train_index], \n",
    "                                                       X_subv[train_index], \n",
    "                                                       X_locc[train_index]], \n",
    "                                                      [X_conn[test_index], \n",
    "                                                       X_surf[test_index], \n",
    "                                                       X_thic[test_index], \n",
    "                                                       X_subv[test_index], \n",
    "                                                       X_locc[test_index]], y_train, y_test)     \n",
    "        \n",
    "        # Store single channel r2\n",
    "        single_r2_scores[i_split, jj,:] = np.array([r2_score(y_test, X_test_2[:, kk]) \\\n",
    "                                                  for kk in range(n_mods)])\n",
    "        \n",
    "        # Define Lasso second classifier\n",
    "        clf_2 = LassoCV(cv=5, random_state=RANDOM_STATE)\n",
    "        \n",
    "        # Fit on the stacked predictions\n",
    "        clf_2.fit(X_train_2, y_train)\n",
    "        \n",
    "        # predict on the stacked predictions\n",
    "        y_pred_comb = clf_2.predict(X_test_2)\n",
    "        \n",
    "        # Compute and store r2 for stacked predictions\n",
    "        r2_comb = r2_score(y_test, y_pred_comb)\n",
    "        comb_r2_scores[i_split, jj] = r2_comb\n",
    "        \n",
    "        # Store the weights for each single prediction\n",
    "        comb_weights[i_split, jj, :] = clf_2.coef_\n",
    "        \n",
    "        #Create dataframe with the predictions\n",
    "        preds_df = pd.DataFrame({'y_pred_conn': X_test_2[:, 0],\n",
    "                                 'y_pred_surf': X_test_2[:, 1],\n",
    "                                 'y_pred_thic': X_test_2[:, 2],\n",
    "                                 'y_pred_subv': X_test_2[:, 3],\n",
    "                                 'y_pred_locc': X_test_2[:, 4],\n",
    "                                 'y_pred_comb': y_pred_comb,\n",
    "                                 'y_test': y_test}\n",
    "                    )\n",
    "        \n",
    "        # Store this\n",
    "        preds_dict[score].append(preds_df)\n",
    "    \n",
    "    print(\"split %d finished\" % i_split)\n",
    "    i_split = i_split + 1\n",
    "\n",
    "# compute final execution time and elapsed time\n",
    "done = time.time()\n",
    "elapsed_time = done - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store these results into hdf5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(opj('../results', 'predictions.hdf5'), 'a') as f_preds:\n",
    "\n",
    "    for score, preds in preds_dict.items():\n",
    "        f_preds.create_dataset(score, \n",
    "                               data = np.asarray([mc_pred.to_numpy()\n",
    "                                                  for mc_pred in preds]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_res = h5py.File(opj('../results', 'scores.hdf5'), 'a')\n",
    "f_res.create_dataset('comb_2_scores', data=comb_r2_scores)\n",
    "f_res.create_dataset('single_r2_scores', data=single_r2_scores)\n",
    "f_res.create_dataset('comb_weights', data = comb_weights)\n",
    "f_res.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
